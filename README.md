# Fast Reconstruction for Deep Learning PET Head Motion Correction

Tianyi Zeng<sup>1</sup>, Ph.D.; Jiazhen Zhang<sup>3</sup>; El√©onore V. Lieffrig<sup>1</sup>, M.Sc.; Zhuotong Cai<sup>1</sup>; Mika Naganawa<sup>1</sup>, Ph.D.; Fuyao Chen<sup>2</sup>; Chenyu You<sup>3</sup>; Yihuan Lu<sup>5</sup>, Ph.D.; John A. Onofrey<sup>1,2,4</sup>, Ph.D.

[Cite this article](#cite-this-article)
to do

## Abstract

Head motion correction is an essential component of brain PET imaging, in which even motion of small magnitude can greatly degrade image quality and introduce artifacts. 
Building upon previous work, we propose a new head motion correction framework taking fast reconstructions as input. 
The main characteristics of the proposed method are: (i) the adoption of a high-resolution short-frame fast reconstruction workflow; (ii) the development of a novel encoder for PET data representation extraction; and (iii) the implementation of data augmentation
techniques. Ablation studies are conducted to assess the individual contributions of each of these design choices. 
Furthermore, multi-subject studies are conducted on an 18F-FPEB dataset, and the method performance is qualitatively and quantitatively evaluated by MOLAR reconstruction study and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation. Additionally, we also compared our method with a conventional intensity-based registration method.
Our results demonstrate that the proposed method outperforms other methods on all subjects, and can accurately estimate motion for sub-
jects out of the training set. 

## Author information

**Affiliations**

<sup>1</sup>	Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA 

<sup>2</sup>	Department of Biomedical Engineering, Yale University, New Haven, CT, USA

<sup>3</sup>	Department of Electrical Engineering, Yale University, New Haven, CT, USA

<sup>4</sup>	Department of Urology, Yale University, New Haven, CT, USA

<sup>5</sup>  United Imaging, Shanghai, China

## Cite this article
to do

### DOI
to do
